{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9636ff43-b756-45cd-9c0e-884c5434a6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/bojansimoski/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/bojansimoski/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/bojansimoski/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/bojansimoski/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import string\n",
    "from nltk import FreqDist\n",
    "from nltk import bigrams, trigrams, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from textstat.textstat import textstatistics \n",
    "from collections import Counter\n",
    "from chat_analysis import *\n",
    "import os\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import zipfile\n",
    "import configparser\n",
    "import numpy as np\n",
    "import inspect\n",
    "import sys\n",
    "# access parent directory from notebooks directory\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0, parentdir)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502a1ae7-5c5d-461f-aa5e-a7eb38b56ea7",
   "metadata": {},
   "source": [
    "### Notebook for preprocessing of participants chat files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba085a39-17ee-4327-9327-3d1148cc4c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "# Read the configuration file\n",
    "config.read('config.ini')\n",
    "url = config.get('credentials', 'surfdrive_url_movez_chat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2a26a3-fe4e-4f3d-b35f-0400f102c8d2",
   "metadata": {},
   "source": [
    "#### This code preprocesses the whatsapp files, creates a separate csv per participant, with a datetime, username, message format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7dc75423-6bde-4079-bf8a-fb2f4fcb76c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t4/v59wrn6n2ws6kl1t9zh1q_mc0000gn/T/ipykernel_2009/3237678970.py:61: UserWarning: Parsing dates in %d/%m/%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df_chats['datetime'] = pd.to_datetime(df_chats['datetime'], errors='coerce')\n",
      "/var/folders/t4/v59wrn6n2ws6kl1t9zh1q_mc0000gn/T/ipykernel_2009/3237678970.py:61: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_chats['datetime'] = pd.to_datetime(df_chats['datetime'], errors='coerce')\n",
      "/var/folders/t4/v59wrn6n2ws6kl1t9zh1q_mc0000gn/T/ipykernel_2009/3237678970.py:61: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_chats['datetime'] = pd.to_datetime(df_chats['datetime'], errors='coerce')\n",
      "/var/folders/t4/v59wrn6n2ws6kl1t9zh1q_mc0000gn/T/ipykernel_2009/3237678970.py:61: UserWarning: Parsing dates in %d-%m-%Y %H:%M format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df_chats['datetime'] = pd.to_datetime(df_chats['datetime'], errors='coerce')\n",
      "/var/folders/t4/v59wrn6n2ws6kl1t9zh1q_mc0000gn/T/ipykernel_2009/3237678970.py:61: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_chats['datetime'] = pd.to_datetime(df_chats['datetime'], errors='coerce')\n",
      "/var/folders/t4/v59wrn6n2ws6kl1t9zh1q_mc0000gn/T/ipykernel_2009/3237678970.py:93: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_summary = pd.concat([df_summary, pd.DataFrame([{'username': username, 'word_count_median': df['word_count'].median(),\n",
      "/var/folders/t4/v59wrn6n2ws6kl1t9zh1q_mc0000gn/T/ipykernel_2009/3237678970.py:82: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['username'] = df['username'].map(df_users.set_index('username')['index'])\n",
      "/var/folders/t4/v59wrn6n2ws6kl1t9zh1q_mc0000gn/T/ipykernel_2009/3237678970.py:82: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['username'] = df['username'].map(df_users.set_index('username')['index'])\n"
     ]
    }
   ],
   "source": [
    "output_chat_data_folder_path = 'output_chat_data/'\n",
    "# Regular expression pattern for parsing each txt file line\n",
    "pattern_rest = re.compile(rb'\\[((\\d{2}/\\d{2}/\\d{4})|(\\d{2}\\.\\d{2}\\.\\d{2})), (\\d{2}:\\d{2}:\\d{2})\\] (.*?): (.*)\\r\\n')\n",
    "pattern_te = re.compile(rb'(\\d{1,2}/\\d{1,2}/\\d{2,4},? \\d{2}:\\d{2}|\\d{2}-\\d{2}-\\d{4} \\d{2}:\\d{2}) - (.+?): (.*)\\n')\n",
    "# Send an HTTP GET request to the URL (\\d{2}-\\d{2}-\\d{4} \\d{2}:\\d{2}) \n",
    "response = requests.get(url)\n",
    "# dictionary with all the individuals dataframes\n",
    "df_dict = {}\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Create a BytesIO object to hold the downloaded ZIP file content\n",
    "    zip_content = BytesIO(response.content)\n",
    "    csv_file_to_read = 'movez_chats/movez_participants.csv'\n",
    "    # Use the zipfile module to extract the contents\n",
    "    with zipfile.ZipFile(zip_content, 'r') as zip_ref:\n",
    "        zip_ref.extract(csv_file_to_read)\n",
    "        #extract the participants anonymization file\n",
    "        df_users = pd.read_csv(csv_file_to_read)\n",
    "\n",
    "        for file_info in zip_ref.infolist():\n",
    "            # Lists to store the extracted data from file\n",
    "            datetimes = []\n",
    "            usernames = []\n",
    "            messages = []\n",
    "            emojis = []\n",
    "            # Check if the file has a .txt extension\n",
    "            if file_info.filename.endswith('.txt'):\n",
    "                # Extract the content of the text file\n",
    "                with zip_ref.open(file_info.filename) as txt_file:\n",
    "                    # Read and print the content of the text file line by line\n",
    "                    if file_info.filename.endswith('Thabo.txt') or file_info.filename.endswith('Ying.txt'):\n",
    "                        for line in txt_file:\n",
    "                                match = pattern_te.match(line)\n",
    "                                if match:\n",
    "                                    group = match.groups()\n",
    "                                    datetimes.append(group[0].decode('utf-8'))\n",
    "                                    usernames.append(group[1].decode('utf-8'))\n",
    "                                    messages.append(group[2].decode('utf-8'))\n",
    "                                    emojis.append('')\n",
    "                    else:\n",
    "                        for line in txt_file:\n",
    "                            match = pattern_rest.match(line)\n",
    "                            if match:\n",
    "                                group = match.groups()\n",
    "                                # Combine date and time into a single string\n",
    "                                datetime_str = group[0].decode('utf-8') + ' ' + group[3].decode('utf-8')\n",
    "                                datetimes.append(datetime_str)\n",
    "                                usernames.append(group[4].decode('utf-8'))\n",
    "                                messages.append(group[5].decode('utf-8'))\n",
    "                                emojis.append('')                            \n",
    "                            \n",
    "                \n",
    "                    # Creating a DataFrame\n",
    "                    df_chats = pd.DataFrame({\n",
    "                        'datetime': datetimes,\n",
    "                        'username': usernames,\n",
    "                        'message': messages,\n",
    "                        'emojis': emojis\n",
    "                    })\n",
    "                                \n",
    "                    df_chats['datetime'] = pd.to_datetime(df_chats['datetime'], errors='coerce')\n",
    "                    # Filter rows where datetime is greater than January 25, 2024 - start of the data donation experiment\n",
    "                    df_chats = df_chats[df_chats['datetime'] > '2024-01-25']\n",
    "                    # Applied lexical functions per message\n",
    "                    df_chats[['message', 'emojis']] = df_chats['message'].apply(extract_emojis).tolist()\n",
    "                    df_chats['word_count'] = df_chats['message'].apply(word_count)\n",
    "                    df_chats['punctuation_count'] = df_chats['message'].apply(punctuation_count)\n",
    "                    df_chats['readability_score'] = df_chats['message'].apply(readability_score)\n",
    "                    df_chats['lexical_density'] = df_chats['message'].apply(lexical_density)\n",
    "                    \n",
    "                    # Extracting unique usernames\n",
    "                    unique_usernames = df_chats['username'].unique()\n",
    "                    \n",
    "                    # Creating a dictionary to hold the DataFrames for each unique username\n",
    "                    df_dict.update({username: df_chats[df_chats['username'] == username] for username in unique_usernames})\n",
    "    \n",
    "    df_summary = pd.DataFrame(columns=['username','word_count_median','punctuation_count_avg','vocabulary_diversity','emoji_avg','lexical_density','readability_score'])\n",
    "    \n",
    "    #creating corpus level features here\n",
    "    for df in df_dict.values():\n",
    "        # Extracting the username from the first row of the DataFrame\n",
    "        df['username'] = df['username'].map(df_users.set_index('username')['index'])\n",
    "        username = df['username'].iloc[0]\n",
    "        # Sanitize the username to ensure it's safe for use as a file name\n",
    "        sanitized_username = \"\".join([c for c in username if c.isalpha() or c.isdigit() or c==' ']).rstrip()\n",
    "        \n",
    "        # Lexical features applied on the whole corpus\n",
    "        all_messages = '. '.join(df['message'].astype(str))\n",
    "        # words = word_tokenize(all_messages)\n",
    "        # unique_words = set(words)\n",
    "        # ttr = len(unique_words) / len(words) if words else 0\n",
    "\n",
    "        df_summary = pd.concat([df_summary, pd.DataFrame([{'username': username, 'word_count_median': df['word_count'].median(),\n",
    "                                        'punctuation_count_avg': round(df['punctuation_count'].mean(),2), \n",
    "                                        'vocabulary_diversity': round(vocabulary_diversity(all_messages),2),\n",
    "                                        'emoji_avg': df['emojis'].count()/ df.shape[0], 'lexical_density' : round(df['lexical_density'].mean(),2),\n",
    "                                        'readability_score' : round(df['readability_score'].mean(),2) }])], ignore_index=True)\n",
    "\n",
    "        # saved to separate csv files\n",
    "        df_grams = get_top_ngrams(all_messages, n=2)\n",
    "        df_grams.to_csv(output_chat_data_folder_path + username + '_ngram.csv',index=False)\n",
    "        \n",
    "        df_pos_distribution = pos_distribution(all_messages)\n",
    "        df_pos_distribution.to_csv(output_chat_data_folder_path + username + '_pos_distribution.csv',index=False)\n",
    "\n",
    "        df_top_pos = get_top_words_by_pos(all_messages)\n",
    "        df_top_pos.to_csv(output_chat_data_folder_path + username + '_top_10_pos.csv',index=False)\n",
    "        \n",
    "        df_word_length_distribution = word_length_distribution(all_messages)\n",
    "        df_word_length_distribution.to_csv(output_chat_data_folder_path + username + '_word_distribution.csv',index=False)\n",
    "\n",
    "        \n",
    "        # Constructing the filename\n",
    "        filename = output_chat_data_folder_path + f'{sanitized_username}_chat_llm.csv'\n",
    "        filename_whole_corpus = output_chat_data_folder_path +  f'{sanitized_username}_all.txt'\n",
    "        f = open(filename_whole_corpus,'w')\n",
    "        f.write(all_messages) #Give your csv text here.\n",
    "        ## Python will convert \\n to os.linesep\n",
    "        f.close()\n",
    "        # Saving the DataFrame to a CSV file\n",
    "        df = df.reset_index(drop=True)\n",
    "        df = df.reset_index(names='messageID')\n",
    "        df.to_csv(filename,index=False)\n",
    "\n",
    "        df_summary.to_csv(output_chat_data_folder_path+'chat_preprocess_summary.csv',index=False)\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to download file. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58c0397a-5fad-4b9e-bc08-bd22a20d0afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "    A  B\n",
      "10  1  4\n",
      "20  2  5\n",
      "30  3  6\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {'A': [1, 2, 3], 'B': [4, 5, 6]}\n",
    "index_values = [10, 20, 30]\n",
    "my_dataframe = pd.DataFrame(data, index=index_values)\n",
    "\n",
    "# Display the original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "print(my_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ee0eeec-0740-4241-9531-07bc1181a8e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    A  B\n",
       "10  1  4\n",
       "20  2  5\n",
       "30  3  6"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4fbe77f-0fae-45ad-a224-bf78d8b46a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>username</th>\n",
       "      <th>message</th>\n",
       "      <th>emojis</th>\n",
       "      <th>word_count</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>readability_score</th>\n",
       "      <th>lexical_density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-30 16:32:55</td>\n",
       "      <td>U9</td>\n",
       "      <td>Hi this is Zhanna!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>108.03</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-30 16:35:25</td>\n",
       "      <td>U9</td>\n",
       "      <td>Hey! how are you?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>116.15</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-30 16:35:52</td>\n",
       "      <td>U9</td>\n",
       "      <td>me too...tired from today already haha</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>107.01</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-30 16:36:48</td>\n",
       "      <td>U9</td>\n",
       "      <td>yes! really looking forward to nijmegen, have ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>95.34</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-30 16:37:27</td>\n",
       "      <td>U9</td>\n",
       "      <td>I used to live there when I first moved to the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>47.13</td>\n",
       "      <td>0.227273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024-01-30 16:37:29</td>\n",
       "      <td>U9</td>\n",
       "      <td>it's so nice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>113.10</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024-01-30 16:38:52</td>\n",
       "      <td>U9</td>\n",
       "      <td>Well I wouldn't say a lot but i know good food...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37</td>\n",
       "      <td>10</td>\n",
       "      <td>74.23</td>\n",
       "      <td>0.377778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2024-01-30 16:38:58</td>\n",
       "      <td>U9</td>\n",
       "      <td>do you have a favorite city in the netherlands?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>83.67</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2024-01-30 16:41:13</td>\n",
       "      <td>U9</td>\n",
       "      <td>Yeaah Amsterdam is so pretty too, I completely...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>78.59</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2024-01-30 16:41:35</td>\n",
       "      <td>U9</td>\n",
       "      <td>It was like from a fairy tale when i first saw...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>18.71</td>\n",
       "      <td>0.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2024-01-30 16:41:36</td>\n",
       "      <td>U9</td>\n",
       "      <td>btw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>119.19</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2024-01-30 16:42:28</td>\n",
       "      <td>U9</td>\n",
       "      <td>yeah exactly, taking things for granted</td>\n",
       "      <td>ðŸ¥²</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>88.74</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2024-01-30 16:42:35</td>\n",
       "      <td>U9</td>\n",
       "      <td>do you like to travel generally?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>95.85</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2024-01-30 16:42:47</td>\n",
       "      <td>U9</td>\n",
       "      <td>Although if you have been in Armenia you proba...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>54.23</td>\n",
       "      <td>0.235294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2024-01-30 16:45:02</td>\n",
       "      <td>U9</td>\n",
       "      <td>I think that's so cool! I know you were there ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33</td>\n",
       "      <td>6</td>\n",
       "      <td>79.91</td>\n",
       "      <td>0.256410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2024-01-30 16:46:12</td>\n",
       "      <td>U9</td>\n",
       "      <td>oooh it's hard to choose but I think Barcelona...</td>\n",
       "      <td>ðŸ˜‚</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.147059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2024-01-30 16:46:59</td>\n",
       "      <td>U9</td>\n",
       "      <td>oh yeah okay, that makes sense lol. if you eve...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>75.55</td>\n",
       "      <td>0.483871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2024-01-30 16:47:04</td>\n",
       "      <td>U9</td>\n",
       "      <td>have you been in barcelona?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>99.91</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2024-01-30 16:49:07</td>\n",
       "      <td>U9</td>\n",
       "      <td>that's so impressive! were you travelling solo?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>102.95</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2024-01-30 16:49:24</td>\n",
       "      <td>U9</td>\n",
       "      <td>oh yeah those are signature lol</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>95.85</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2024-01-30 16:50:00</td>\n",
       "      <td>U9</td>\n",
       "      <td>thats even cooler lol</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>103.97</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2024-01-30 16:50:39</td>\n",
       "      <td>U9</td>\n",
       "      <td>I think I've ever only travelled once or twice...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>2.47</td>\n",
       "      <td>0.419355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2024-01-30 16:50:55</td>\n",
       "      <td>U9</td>\n",
       "      <td>but indonesia is really cool! I can imagine th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>88.74</td>\n",
       "      <td>0.411765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2024-01-30 16:52:55</td>\n",
       "      <td>U9</td>\n",
       "      <td>sounds like a dream vacation to be honest hahaha</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>81.64</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2024-01-30 16:52:59</td>\n",
       "      <td>U9</td>\n",
       "      <td>any travel plans in the future?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>96.86</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2024-01-30 16:53:56</td>\n",
       "      <td>U9</td>\n",
       "      <td>that's super cool</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>108.03</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2024-01-30 16:54:06</td>\n",
       "      <td>U9</td>\n",
       "      <td>I hope to take my partner to Armenia sometime ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>77.58</td>\n",
       "      <td>0.454545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2024-01-30 16:54:18</td>\n",
       "      <td>U9</td>\n",
       "      <td>but nothing else really, there's too many choices</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>81.64</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2024-01-30 16:56:05</td>\n",
       "      <td>U9</td>\n",
       "      <td>aww thanks! I think I want to show him everyth...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>89.04</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2024-01-30 16:56:11</td>\n",
       "      <td>U9</td>\n",
       "      <td>but we also want to go on a hike together</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>89.76</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2024-01-30 16:56:20</td>\n",
       "      <td>U9</td>\n",
       "      <td>armenia is amazing for hiking! especially in t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>98.89</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2024-01-30 16:56:22</td>\n",
       "      <td>U9</td>\n",
       "      <td>do you like hiking?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>107.01</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2024-01-30 16:57:28</td>\n",
       "      <td>U9</td>\n",
       "      <td>yessss, lots! so that also means you can go on...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>88.23</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2024-01-30 16:58:18</td>\n",
       "      <td>U9</td>\n",
       "      <td>hahaha i feel you</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>108.03</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2024-01-30 16:58:29</td>\n",
       "      <td>U9</td>\n",
       "      <td>i love it! but ive only hiked in the snow and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>101.94</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2024-01-30 16:58:39</td>\n",
       "      <td>U9</td>\n",
       "      <td>in the summer i'm scared of bugs</td>\n",
       "      <td>ðŸ¥²</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>96.86</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2024-01-30 16:58:58</td>\n",
       "      <td>U9</td>\n",
       "      <td>but I told myself I'm going to do it in the sp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>52.20</td>\n",
       "      <td>0.291667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2024-01-30 16:59:59</td>\n",
       "      <td>U9</td>\n",
       "      <td>not from my experience...so the worst thing th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61</td>\n",
       "      <td>9</td>\n",
       "      <td>37.28</td>\n",
       "      <td>0.308824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2024-01-30 17:00:08</td>\n",
       "      <td>U9</td>\n",
       "      <td>lol yeah for real</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>108.03</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2024-01-30 17:01:31</td>\n",
       "      <td>U9</td>\n",
       "      <td>yeah indeed!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>112.09</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime username  \\\n",
       "0  2024-01-30 16:32:55       U9   \n",
       "1  2024-01-30 16:35:25       U9   \n",
       "2  2024-01-30 16:35:52       U9   \n",
       "3  2024-01-30 16:36:48       U9   \n",
       "4  2024-01-30 16:37:27       U9   \n",
       "5  2024-01-30 16:37:29       U9   \n",
       "6  2024-01-30 16:38:52       U9   \n",
       "7  2024-01-30 16:38:58       U9   \n",
       "8  2024-01-30 16:41:13       U9   \n",
       "9  2024-01-30 16:41:35       U9   \n",
       "10 2024-01-30 16:41:36       U9   \n",
       "11 2024-01-30 16:42:28       U9   \n",
       "12 2024-01-30 16:42:35       U9   \n",
       "13 2024-01-30 16:42:47       U9   \n",
       "14 2024-01-30 16:45:02       U9   \n",
       "15 2024-01-30 16:46:12       U9   \n",
       "16 2024-01-30 16:46:59       U9   \n",
       "17 2024-01-30 16:47:04       U9   \n",
       "18 2024-01-30 16:49:07       U9   \n",
       "19 2024-01-30 16:49:24       U9   \n",
       "20 2024-01-30 16:50:00       U9   \n",
       "21 2024-01-30 16:50:39       U9   \n",
       "22 2024-01-30 16:50:55       U9   \n",
       "23 2024-01-30 16:52:55       U9   \n",
       "24 2024-01-30 16:52:59       U9   \n",
       "25 2024-01-30 16:53:56       U9   \n",
       "26 2024-01-30 16:54:06       U9   \n",
       "27 2024-01-30 16:54:18       U9   \n",
       "28 2024-01-30 16:56:05       U9   \n",
       "29 2024-01-30 16:56:11       U9   \n",
       "30 2024-01-30 16:56:20       U9   \n",
       "31 2024-01-30 16:56:22       U9   \n",
       "32 2024-01-30 16:57:28       U9   \n",
       "33 2024-01-30 16:58:18       U9   \n",
       "34 2024-01-30 16:58:29       U9   \n",
       "35 2024-01-30 16:58:39       U9   \n",
       "36 2024-01-30 16:58:58       U9   \n",
       "37 2024-01-30 16:59:59       U9   \n",
       "38 2024-01-30 17:00:08       U9   \n",
       "39 2024-01-30 17:01:31       U9   \n",
       "\n",
       "                                              message emojis  word_count  \\\n",
       "0                                  Hi this is Zhanna!    NaN           4   \n",
       "1                                   Hey! how are you?    NaN           4   \n",
       "2              me too...tired from today already haha    NaN           7   \n",
       "3   yes! really looking forward to nijmegen, have ...    NaN          11   \n",
       "4   I used to live there when I first moved to the...    NaN          18   \n",
       "5                                        it's so nice    NaN           3   \n",
       "6   Well I wouldn't say a lot but i know good food...    NaN          37   \n",
       "7     do you have a favorite city in the netherlands?    NaN           9   \n",
       "8   Yeaah Amsterdam is so pretty too, I completely...    NaN           9   \n",
       "9   It was like from a fairy tale when i first saw...    NaN          30   \n",
       "10                                                btw    NaN           1   \n",
       "11           yeah exactly, taking things for granted       ðŸ¥²           6   \n",
       "12                   do you like to travel generally?    NaN           6   \n",
       "13  Although if you have been in Armenia you proba...    NaN          16   \n",
       "14  I think that's so cool! I know you were there ...    NaN          33   \n",
       "15  oooh it's hard to choose but I think Barcelona...      ðŸ˜‚          30   \n",
       "16  oh yeah okay, that makes sense lol. if you eve...    NaN          27   \n",
       "17                        have you been in barcelona?    NaN           5   \n",
       "18    that's so impressive! were you travelling solo?    NaN           7   \n",
       "19                    oh yeah those are signature lol    NaN           6   \n",
       "20                              thats even cooler lol    NaN           4   \n",
       "21  I think I've ever only travelled once or twice...    NaN          27   \n",
       "22  but indonesia is really cool! I can imagine th...    NaN          15   \n",
       "23   sounds like a dream vacation to be honest hahaha    NaN           9   \n",
       "24                    any travel plans in the future?    NaN           6   \n",
       "25                                  that's super cool    NaN           3   \n",
       "26  I hope to take my partner to Armenia sometime ...    NaN          11   \n",
       "27  but nothing else really, there's too many choices    NaN           8   \n",
       "28  aww thanks! I think I want to show him everyth...    NaN          24   \n",
       "29          but we also want to go on a hike together    NaN          10   \n",
       "30  armenia is amazing for hiking! especially in t...    NaN           9   \n",
       "31                                do you like hiking?    NaN           4   \n",
       "32  yessss, lots! so that also means you can go on...    NaN          16   \n",
       "33                                  hahaha i feel you    NaN           4   \n",
       "34  i love it! but ive only hiked in the snow and ...    NaN          12   \n",
       "35                  in the summer i'm scared of bugs       ðŸ¥²           7   \n",
       "36  but I told myself I'm going to do it in the sp...    NaN          21   \n",
       "37  not from my experience...so the worst thing th...    NaN          61   \n",
       "38                                  lol yeah for real    NaN           4   \n",
       "39                                       yeah indeed!    NaN           2   \n",
       "\n",
       "    punctuation_count  readability_score  lexical_density  \n",
       "0                   1             108.03         0.000000  \n",
       "1                   2             116.15         0.166667  \n",
       "2                   3             107.01         0.500000  \n",
       "3                   3              95.34         0.428571  \n",
       "4                   4              47.13         0.227273  \n",
       "5                   1             113.10         0.500000  \n",
       "6                  10              74.23         0.377778  \n",
       "7                   1              83.67         0.300000  \n",
       "8                   1              78.59         0.400000  \n",
       "9                   0              18.71         0.366667  \n",
       "10                  0             119.19         1.000000  \n",
       "11                  1              88.74         0.285714  \n",
       "12                  1              95.85         0.571429  \n",
       "13                  1              54.23         0.235294  \n",
       "14                  6              79.91         0.256410  \n",
       "15                  4               1.45         0.147059  \n",
       "16                  4              75.55         0.483871  \n",
       "17                  1              99.91         0.166667  \n",
       "18                  3             102.95         0.300000  \n",
       "19                  0              95.85         0.500000  \n",
       "20                  0             103.97         0.500000  \n",
       "21                  4               2.47         0.419355  \n",
       "22                  2              88.74         0.411765  \n",
       "23                  0              81.64         0.555556  \n",
       "24                  1              96.86         0.285714  \n",
       "25                  1             108.03         0.250000  \n",
       "26                  0              77.58         0.454545  \n",
       "27                  2              81.64         0.500000  \n",
       "28                  4              89.04         0.285714  \n",
       "29                  0              89.76         0.400000  \n",
       "30                  1              98.89         0.300000  \n",
       "31                  1             107.01         0.200000  \n",
       "32                  2              88.23         0.444444  \n",
       "33                  0             108.03         0.500000  \n",
       "34                  1             101.94         0.384615  \n",
       "35                  1              96.86         0.250000  \n",
       "36                  3              52.20         0.291667  \n",
       "37                  9              37.28         0.308824  \n",
       "38                  0             108.03         0.750000  \n",
       "39                  1             112.09         0.333333  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe1db3bd-2786-4960-aaa6-f14b23dfb51c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot insert messageID, already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t4/v59wrn6n2ws6kl1t9zh1q_mc0000gn/T/ipykernel_2009/3227456406.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'messageID'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/llm-evaluate/lib/python3.11/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, level, drop, inplace, col_level, col_fill, allow_duplicates, names)\u001b[0m\n\u001b[1;32m   6216\u001b[0m                     level_values = algorithms.take(\n\u001b[1;32m   6217\u001b[0m                         \u001b[0mlevel_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_na_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6218\u001b[0m                     )\n\u001b[1;32m   6219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6220\u001b[0;31m                 new_obj.insert(\n\u001b[0m\u001b[1;32m   6221\u001b[0m                     \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6222\u001b[0m                     \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6223\u001b[0m                     \u001b[0mlevel_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/llm-evaluate/lib/python3.11/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, loc, column, value, allow_duplicates)\u001b[0m\n\u001b[1;32m   4927\u001b[0m                 \u001b[0;34m\"'self.flags.allows_duplicate_labels' is False.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4928\u001b[0m             )\n\u001b[1;32m   4929\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_duplicates\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4930\u001b[0m             \u001b[0;31m# Should this be a different kind of error??\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4931\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"cannot insert {column}, already exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4932\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4933\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loc must be int\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4934\u001b[0m         \u001b[0;31m# convert non stdlib ints to satisfy typing checks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot insert messageID, already exists"
     ]
    }
   ],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "df = df.reset_index(names='messageID')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4a5235-e568-41a6-bd16-ab0c1f749e89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm-evaluate)",
   "language": "python",
   "name": "llm-evaluate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
